{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3OlLdVUYnOu"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers soundfile torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "sqclLJvCYrDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ar\")"
      ],
      "metadata": {
        "id": "kgHAIzxMYr0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import Wav2Vec2Processor\n",
        "import torchaudio\n",
        "\n",
        "dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ar\", split=\"train+validation\")\n",
        "\n",
        "def resample_audio(batch):\n",
        "    batch[\"audio\"] = torchaudio.load(batch[\"path\"])[0][0].numpy()\n",
        "    return batch\n",
        "\n",
        "dataset = dataset.map(resample_audio)\n"
      ],
      "metadata": {
        "id": "Ut993IsvYs9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to clean and normalize text\n",
        "def clean_text(batch):\n",
        "    batch[\"text\"] = re.sub(\"[^ุก-ู ]\", \"\", batch[\"sentence\"])  # Remove non-Arabic chars\n",
        "    return batch\n",
        "\n",
        "dataset = dataset.map(clean_text, num_proc=4)\n"
      ],
      "metadata": {
        "id": "lnRcOdfrYujO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\", sampling_rate=16000)\n",
        "\n",
        "small_dataset = dataset.select(range(int(len(dataset)*.5)))\n",
        "\n",
        "def batch_tokenize(batch):\n",
        "    batch[\"input_values\"] = processor(\n",
        "        batch[\"audio\"],\n",
        "        sampling_rate=16000,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    ).input_values\n",
        "    batch[\"labels\"] = processor.tokenizer(batch[\"text\"], padding=True).input_ids\n",
        "    return batch\n",
        "\n",
        "small_dataset = small_dataset.map(batch_tokenize, batched=True, remove_columns=[\"audio\", \"sentence\"])\n"
      ],
      "metadata": {
        "id": "D75FGP4hYw19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2ForCTC, TrainingArguments, Trainer\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\",\n",
        "    vocab_size=len(processor.tokenizer),\n",
        "    ctc_loss_reduction=\"mean\",\n",
        "    pad_token_id=processor.tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./wav2vec2-ar\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=1e-4,\n",
        "    warmup_steps=500,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=processor.data_collator,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    tokenizer=processor.feature_extractor\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "m1kZYP3NYzL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "tmcuFw2MczSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ar\", split=\"test\")\n",
        "test_dataset = test_dataset.map(resample_audio).map(clean_text).map(tokenize)\n",
        "\n",
        "results = trainer.evaluate(test_dataset)\n",
        "print(f\"Test Results: {results}\")\n"
      ],
      "metadata": {
        "id": "3gBzpA50cy69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ar\", split=\"test\")\n",
        "test_dataset = test_dataset.map(resample_audio).map(clean_text).map(tokenize)\n",
        "\n",
        "results = trainer.evaluate(test_dataset)\n",
        "print(f\"Test Results: {results}\")\n"
      ],
      "metadata": {
        "id": "on2gx0J7c2E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "\n",
        "model_path = \"./model\"\n",
        "processor = Wav2Vec2Processor.from_pretrained(model_path)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_path)\n",
        "\n",
        "audio_path = \"my_voice_test.wav\"  # Replace with your .wav file path\n",
        "waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "if sample_rate != 16000:\n",
        "    waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
        "\n",
        "input_values = processor(waveform.squeeze(), sampling_rate=16000, return_tensors=\"pt\").input_values\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(input_values).logits\n",
        "\n",
        "# Decode prediction\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "transcription = processor.batch_decode(predicted_ids)\n",
        "\n",
        "print(\"Transcription:\", transcription[0])\n"
      ],
      "metadata": {
        "id": "EVCBuFOedMqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solved Hussain Yafei"
      ],
      "metadata": {
        "id": "b0t1tX1_dnMF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UAu_mfy_dv_d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}